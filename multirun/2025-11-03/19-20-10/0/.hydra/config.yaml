model:
  name: mistralai/Mixtral-8x7B-Instruct-v0.1
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
training:
  output_dir: ./models/default
  num_train_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 0.0001
  fp16: false
  bf16: true
  logging_steps: 10
  save_strategy: steps
  save_steps: 100
  warmup_steps: 50
  optim: paged_adamw_8bit
  max_steps: 100
  gradient_checkpointing: true
  report_to: wandb
dataset:
  name: timdettmers/openassistant-guanaco
  split: train[:1000]
wandb:
  project: finetuning
  name: ${model.name}-${training.learning_rate}
